{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "* Logistic Regression is one of the basic and popular algorithm to solve a classification problem. \n",
    "* It is named as ‘Logistic Regression’, because it’s underlying technique is quite the same as Linear Regression. \n",
    "* The term “Logistic” is taken from the Logit function that is used in this method of classification.\n",
    "* Logistic Regression is used when the dependent variable(target) is categorical (you need to encode the categorical variable into a numeric value)\n",
    "* For multi class problems you can use Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why not use Linear Regression?\n",
    "\n",
    "Consider an example of where we have data of tumor size vs its malignancy. As it is a classification problem that either a patient is maligant or not. If we plot, we can see, all the values will lie on 0 and 1. And if we fit best found regression line, by assuming the threshold at 0.5, the graph might look like below\n",
    "\n",
    "<img src='images/logit1.png' alt='logit1' style=\"width: 500px;\"/>\n",
    "\n",
    "We can decide the point on the x axis from where all the values lie to its left side are considered as negative class and all the values lie to its right side are positive class.\n",
    "\n",
    "<img src='images/logit2.jpeg' alt='logit2' style=\"width: 500px;\"/>\n",
    "\n",
    "But what if there is an outlier in the data. Things would get pretty messy. For example, for 0.5 threshold.\n",
    "\n",
    "<img src='images/logit3.png' alt='logit3' style=\"width: 500px;\"/>\n",
    "\n",
    "If we fit best found regression line, it still won’t be enough to decide any point by which we can differentiate classes. It will put some positive class examples into negative class. The green dotted line (Decision Boundary) is dividing malignant tumors from benign tumors but the line should have been at a yellow line which is clearly dividing the positive and negative examples. So just a single outlier is disturbing the whole linear regression predictions. And that is where logistic regression comes into a picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "As discussed earlier, to deal with outliers, Logistic Regression uses Sigmoid function.\n",
    "An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a Sigmoid function, which takes any real value between zero and one.\n",
    "\n",
    "Step functions enforce some threshold on a variable e.g.: discard anything below a certain value. Given $u(x)$ any value equal to or greater than $0$ would trigger a positive output ($1$ in this case), everything else just equals zero. With $3u(x-8)$ everything equal to or greater than $8$ triggers a non-zero output ($3$ in this case).\n",
    "\n",
    "$$u(x) \\begin{cases} 0 & \\mbox{if} & x < 0 \\ 1 & \\mbox{if} & x \\geq 0 \\ \\end{cases}$$\n",
    "\n",
    "Step functions are like switches. We could utilise them to trigger a signal whenever a threshold is crossed. Imagine opening the floodgates whenever the water level crosses a certain point or turning of the electric lighting when the ambient light brightness exceeds a given measure.\n",
    "\n",
    "Perceptrons, binary classifiers and the neurons in artificial neural networks require an activation function in order to produce any output. The step function is a valid option for the activation function but poses a challenge in analysis because of the jump disconuity at $x=0$.\n",
    "\n",
    "At $x=0$ the derivative is undefined ($\\frac{\\infty}{0}$) while the derivative is zero for the remainder of the domain.\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "The sigmoid function $\\sigma(x)$, because of its differentiability, is sometimes used as an alternative to the step function $u(x)$.\n",
    "\n",
    "Some may choose to introduce a weight $\\beta$ to the input variables in order to obtain a sigmoid curve more reminiscent of the step we're trying to mimic. By tweaking the $\\beta$ term, one can obtain a result that approaches the step function, yet remains differentiable throughout its entire domain.\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x\\beta}}$$\n",
    "\n",
    "\n",
    "<img src='images/logit4.png' alt='logit4' style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression on Breast Cancer Data\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
    "\n",
    "Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "a) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "All feature values are recoded with four significant digits.\n",
    "\n",
    "Missing attribute values: none\n",
    "\n",
    "Class distribution: 357 benign, 212 malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/data.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Unnamed: 32','id']\n",
    "df = df.drop(drop_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "y = df.diagnosis.values\n",
    "x_data = df.drop(['diagnosis'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (x_data -np.min(x_data))/(np.max(x_data)-np.min(x_data)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train:  (30, 483)\n",
      "x test:  (30, 86)\n",
      "y train:  (483,)\n",
      "y test:  (86,)\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)\n",
    "\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "print(\"x train: \",x_train.shape)\n",
    "print(\"x test: \",x_test.shape)\n",
    "print(\"y train: \",y_train.shape)\n",
    "print(\"y test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "# lets initialize parameters\n",
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.full((dimension,1),0.01)\n",
    "    b = 0.0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "def sigmoid(z):\n",
    "    y_head = 1/(1+np.exp(-z))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward and backward\n",
    "\n",
    "def forward_backward_propagation(w,b,x_train,y_train):\n",
    "    # forward propagation\n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_head = sigmoid(z)\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = (np.sum(loss))/x_train.shape[1]\n",
    "    # backward propagation\n",
    "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n",
    "    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating(learning) parameters\n",
    "def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    # updating(learning) parameters is number_of_iterarion times\n",
    "    for i in range(number_of_iterarion):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        # lets update\n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    # we update(learn) parameters weights and bias\n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def predict(w,b,x_test):\n",
    "    # x_test is a input for forward propagation\n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
    "    # initialize\n",
    "    dimension =  x_train.shape[0]  # that is 4096\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    # do not change learning rate\n",
    "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.692836\n",
      "Cost after iteration 10: 0.498576\n",
      "Cost after iteration 20: 0.404996\n",
      "Cost after iteration 30: 0.350059\n",
      "Cost after iteration 40: 0.313747\n",
      "Cost after iteration 50: 0.287767\n",
      "Cost after iteration 60: 0.268114\n",
      "Cost after iteration 70: 0.252627\n",
      "Cost after iteration 80: 0.240036\n",
      "Cost after iteration 90: 0.229543\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 94.40993788819875 %\n",
      "test accuracy: 94.18604651162791 %\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using sklearn package in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.9651162790697675 \n",
      "train accuracy: 0.9668737060041408 \n"
     ]
    }
   ],
   "source": [
    "# sklearn\n",
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(random_state = 42, max_iter= 150, solver='liblinear')\n",
    "print(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\n",
    "print(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
