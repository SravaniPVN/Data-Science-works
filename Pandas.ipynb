{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to NumPy**\n",
    "\n",
    "NumPy (short form for Numerical Python) is the most fundamental package designed for scientific computing and data analysis. Most of the other packages such as pandas, statsmodels are built on top of it, and is an important package to know and learn about. At the heart of NumPy is a data structure called **ndarray**. ndarray is a basically a multi-dimensional array that is built specifically for the purpose of numerical data analysis. Python also has array capabilities, but they are more generic. The advantage of using ndarray is that processing is extremely efficient and fast. \n",
    "\n",
    "You can perform standard mathematical operations on either individual elements or complete array. The range of functions covered is linear algebra, statistical operations, and other specialized mathematical operations. For our purpose, we need to know about ndarray and the range of mathematical functions that are relevant to our research purpose. If you already know languages such as C, Fortran, then you can integrate NumPy code with code written in these languages and can pass NumPy arrays seamlessly. \n",
    "\n",
    "From an overall perspective, understanding of NumPy will help us in using pandas effectively as it is built on top of NumPy and frequently we will also be using functions of NumPy in research work. In the current session, we will only look at some of the most important features of NumPy. For a full listing of NumPy features, please visit http://wiki.scipy.org/Numpy_Example_List .\n",
    "\n",
    "Possible application of NumPy package in research work are:\n",
    "\n",
    "+ Algorithmic operations such as sorting, grouping and set operations\n",
    "+ Performing repetitive operations on whole arrays of data without using loops\n",
    "+ Data merging and alignment operations\n",
    "+ Data indexing, filtering, and transformation on individual elements or whole arrays\n",
    "+ Data summarization and descriptive statistics\n",
    "\n",
    "**Installing NumPy**\n",
    "\n",
    "In order to check if NumPy is installed, go to Package Manager and type NumPy. You will get a list of packages with names closely matching to NumPy. For our purpose, we need to focus on package named numpy 1.xx. If the package is not installed, click on Install. \n",
    "\n",
    "**Importing NumPy**\n",
    "\n",
    "In order to be able to use NumPy, first import it using import statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above statement will import all of NumPy into your workspace. For starters its good, but if you are doing performance intensive work, then saving space is of importance. In such cases, you can import specific modules of NumPy by using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ndarray\n",
    "The most important data structure in NumPy is an n-dimensional array object. Using ndarray, you can store large multidimensional datasets in Python. Being an array, you can perform mathematical operations on these arrays either one element at a time or on complete arrays without using loops. The way to initialize an array object is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = array((1,2,3,4,5))    #initializes an array a and assigns values to it\n",
    "b = array((10,20,30,40,50)) # initializes another array b\n",
    "print a \n",
    "print b\n",
    "print(a+b) \n",
    "print (a+5) \n",
    "print a**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = array(np.arange(15))   #arange function here works as a sequence or counter\n",
    "anarray = array(np.arange(1,15,2)) \n",
    "onemorearray = array(np.linspace(1,10,15)) \n",
    "print(c)\n",
    "print(anarray)\n",
    "print(onemorearray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each ndarray are associated two attributes: shape of the array, and type of the array. The shape of the array tells you about dimensionality of the array (rows and columns), and type of the array tells you about the data type contained in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array((32,45,123,756,23,2123))\n",
    "print(data.shape)\n",
    "print(data.dtype)\n",
    "print(data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [[1,2,3,4],[5,6,7,8]]\n",
    "arr2 = np.array(data2)\n",
    "print(arr2)\n",
    "arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((5,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.eye(5) # creates a 5*5 identity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(array([1,3,5,3,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas**\n",
    "\n",
    "pandas is the primary package for performing data analysis tasks in Python. pandas derives its name from panel data analysis and is the fundamental package that provides relational data structures (think Excel, SQL type) and a host of capabilities to play with those data structures. It is the most widely used package in Python for data analysis tasks, and is very good to work with cross sectional, time series, and panel data analysis. Python sits on top of NumPy and can be used with NumPy arrays and the functions in NumPy. How is pandas suited for a researcher’s needs:\n",
    "\n",
    "+ Has a tabular data structure that can hold both homogenous and heterogenous data.\n",
    "+ Very good indexing capabilities that makes data alignment and merging easy.\n",
    "+ Good time series functionality. No need to use different data structures for time series and cross sectional data. Allows for both ordered and unordered time-series data.\n",
    "+ A host of statistical functions developed around NumPy and pandas that makes a researcher’s task easy and fast.\n",
    "+ Programming is lot simpler and faster.\n",
    "+ Easily handles data manipulation and cleaning.\n",
    "+ Easy to expand and shorten data sets. Comprehensive merging, joins, and group by functionality to join multiple data sets.\n",
    "\n",
    "**Installing pandas** \n",
    "\n",
    "In order to check if pandas is installed, go to Package Manager and type pandas. By default, pandas already comes installed with a distribution of Canopy. If the package is not installed, click on Install.\n",
    "\n",
    "**Importing pandas**\n",
    "\n",
    "In order to be able to use NumPy, first import it using import statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #this will import pandas into your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  #we will be using numpy functions so import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Structures in pandas**\n",
    "\n",
    "There are two basic data structures in pandas: Series and DataFrame\n",
    "\n",
    "**Series:** It is similar to a NumPy 1-dimensional array. In addition to the values that are specified by the programmer, pandas attaches a label to each of the values. If the labels are not provided by the programmer, then pandas assigns labels ( 0 for first element, 1 for second element and so on). A benefit of assigning labels to data values is that it becomes easier to perform manipulations on the dataset as the whole dataset becomes more of a dictionary where each value is associated with a label. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series1 = pd.Series([10,20,30,40])\n",
    "series1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series1.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to specify custom index values rather than the default ones provided, you can do so using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series2 = pd.Series([10,20,30,40,50], index=['one','two','three','four','five'])\n",
    "series2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ways of accesing elements in a Series object are similar to what we have seen in NumPy, and you can perform NumPy operations on Series data arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series2['three']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series2[['one', 'three', 'five']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series2[[0,1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series2 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series2 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series2[series2>30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(series2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a dictionary, you can create a Series data structure from that dictionary. Suppose you are interested in EPS values for firms and the values come from different sources and is not clean. In that case you dont have to worry about cleaning and aligning those values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [90, 91, 92, 93, 94, 95]\n",
    "f1 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11}\n",
    "firm1 = pd.Series(f1,index=years)\n",
    "firm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = {90:14,92:9, 93:13, 94:5}\n",
    "firm2 = pd.Series(f2,index=years)\n",
    "firm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = {93:10, 94:12, 95: 13}\n",
    "firm3 = pd.Series(f3,index=years)\n",
    "firm3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN stands for missing or NA values in pandas. Make use of isnull() function to find out if there are any missing values in the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(firm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key feature of Series data is structures is that you don't have to worry about data alignment. For example, if we have run a word count program on two different files and we have the following data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {'finance': 10, 'earning': 5, 'debt':8}\n",
    "dict2 = {'finance' : 8, 'compensation':4, 'earning': 9}\n",
    "count1 = pd.Series(dict1)\n",
    "count2 = pd.Series(dict2)\n",
    "print count1\n",
    "count2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to calculate the sum of common words in combined files, then we dont have to worry about data alignment. If we want to include all words, then we can take care of NaN values and compute the sum. By default, Series data structure ignores NaN values. NaN values stand for missing data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1+count2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Frame**\n",
    "\n",
    "DataFrame is a tabular data structure in which data is laid out in rows and column format (similar to a CSV and SQL file), but it can also be used for higher dimensional data sets. The DataFrame object can contain homogenous and heterogenous values, and can be thought of as a logical extension of Series data structures. In contrast to Series, where there is one index, a DataFrame object has one index for column and one index for rows. This allows flexibility in accessing and manipulating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'price':[95, 25, 85, 41, 78],\n",
    "                     'ticker':['AXP', 'CSCO', 'DIS', 'MSFT', 'WMT'],\n",
    "                     'company':['American Express', 'Cisco', 'Walt Disney','Microsoft', 'Walmart']})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a column is passed with no values, it will simply have NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access a column, simply mention the column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ix[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ix[data.ticker=='DIS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to add additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Year'] = 2014\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pricesquared'] = data.price**2\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['pricesquared']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pricesquared'] = np.NaN\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sequence'] = np.arange(1,6)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = data.drop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [90, 91, 92, 93, 94, 95]\n",
    "f1 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11}\n",
    "firm1 = pd.Series(f1,index=years)\n",
    "firm1\n",
    "f2 = {90:14,92:9, 93:13, 94:5}\n",
    "firm2 = pd.Series(f2,index=years)\n",
    "firm2\n",
    "f3 = {93:10, 94:12, 95: 13}\n",
    "firm3 = pd.Series(f3,index=years)\n",
    "firm3\n",
    "df1 = pd.DataFrame(columns=['Firm1','Firm2','Firm3'],index=years)\n",
    "df1\n",
    "df1.Firm1 = firm1\n",
    "df1.Firm2 = firm2\n",
    "df1.Firm3 = firm3\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df1.T\n",
    "dft\n",
    "del dft[90]\n",
    "dft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass a number of data structures to DataFrame such as a ndarray, lists, dict, Series, and another DataFrame. You can also reindex to confirm to data to a new index. Reindexing is a powerful feature that allows you to access data in a number of different ways, and also to confirm data to some new time series or other index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf1 = df1.reindex([88,89,90,91,92,93,94,95,96,97,98])\n",
    "reindexdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years1 = [90, 91, 92, 93, 94, 95]\n",
    "f4 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11}\n",
    "firm4 = pd.Series(f4,index=years)\n",
    "f5 = {90:14,91:12, 92:9, 93:13, 94:5, 95:8}\n",
    "firm5 = pd.Series(f5,index=years)\n",
    "f6 = {90:8, 91: 9, 92:9,93:10, 94:12, 95: 13}\n",
    "firm6 = pd.Series(f6,index=years)\n",
    "df2 = pd.DataFrame(columns=['Firm1','Firm2','Firm3'],index=years1)\n",
    "df2.Firm1 = firm4\n",
    "df2.Firm2 = firm5\n",
    "df2.Firm3 = firm6\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf2 = df2.reindex([88,89,90,91,92,93,94,95,96,97,98], fill_value=0)\n",
    "reindexdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you have backfill (bfill) method to fill values backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf3 = df2.reindex([88,89,90,91,92,93,94,95,96,97,98], method='ffill')\n",
    "reindexdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf1+reindexdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf1.add(reindexdf3, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use NumPy functions inside DataFrame objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(np.random.randn(3,3),columns=['one','two','three'])\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x:x.max()-x.min()\n",
    "dataframe.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.apply(f,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda x: x - np.mean(x)\n",
    "dataframe.apply(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return pd.Series([np.mean(x), x.max(), x.min()], index=['mean','max','min'])\n",
    "dataframe.apply(f,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(np.random.randn(3,3),columns=['one','two','three'])\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.sort_index(by='one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.sort_index(by=['one','two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.cumsum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have non-numeric data, then applying describe function would produce statistics such as count, unique, frequency. In addition to this, you can also calculate skewness (skew), kurtosis (kurt), percent changes, difference, and other statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Data**\n",
    "\n",
    "pandas have a number of features to deal with missing data. We have seen an example of the case of descriptive statistics, where missing values are not taken into account while calculating the descriptive statistics. Missing data is denoted by NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [90, 91, 92, 93, 94, 95]\n",
    "f1 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11}\n",
    "firm1 = pd.Series(f1,index=years)\n",
    "firm1\n",
    "f2 = {90:14,92:9, 93:13, 94:5}\n",
    "firm2 = pd.Series(f2,index=years)\n",
    "firm2\n",
    "f3 = {93:10, 94:12, 95: 13}\n",
    "firm3 = pd.Series(f3,index=years)\n",
    "firm3\n",
    "df3 = pd.DataFrame(columns=['Firm1','Firm2','Firm3'],index=years)\n",
    "df3\n",
    "df3.Firm1 = firm1\n",
    "df3.Firm2 = firm2\n",
    "df3.Firm3 = firm3\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadeleted = firm2.dropna()\n",
    "nadeleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of DataFrame, if you use dropna, it deletes entire row by default. Another way is to drop only those rows that are all NA. If you want to drop columns, pass axis=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf3 = df3.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean2 = df3.dropna(how='all')\n",
    "clean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columndrop = df3.dropna(axis=1)\n",
    "columndrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholddf = df3.dropna(thresh=2)\n",
    "thresholddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna1 = df3.fillna(0)\n",
    "fillna1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna2 = df3.fillna({'Firm1':8, 'Firm2': 10, 'Firm3':14})\n",
    "fillna2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna3 = df3.fillna(method='ffill')\n",
    "fillna3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna4 = df3.fillna(method='bfill',limit=2)\n",
    "fillna4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna5 = df3.fillna(df3.mean())\n",
    "fillna5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hierarchical Indexing**\n",
    "\n",
    "Hierarchical indexing allows you to have index on an index (multiple index). It is an important feature of pandas using which you can select subsets of data and perform independent analyses on them. For example, suppose you have firm prices data and the data is indexed by firm name. On top of that, you can index firms by industry. Thus, industry becomes an index on top of firms. You can then perform analyses either on individual firm, or on group of firms in an industry, or on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data = pd.Series(np.random.randn(10),index=[['Ind1','Ind1','Ind1','Ind1','Ind2','Ind2','Ind2','Ind3','Ind3','Ind3'],\n",
    "                                              [1,2,3,4,1,2,3,1,2,3]])\n",
    "h_i_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data['Ind3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data['Ind1':'Ind3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data[['Ind1','Ind3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data.unstack().stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data.sum(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data.sum(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IO in pandas**\n",
    "\n",
    "In this section, we will focus on I/O from text files, csv, excel, and sql files as well as getting data from web such as Yahoo! Finance. Using functions in pandas, you can read data as a DataFrame object. \n",
    "\n",
    "**Reading a csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roedatacsv = pd.read_csv('/Users/peeyushtaori/Desktop/Python/roedata.csv')\n",
    "#roedatacsv\n",
    "roedatacsv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file does not have a header, then you can either let pandas assign default headers or you can specify custom headers. If you want industry name to be the index of DataFrame, you can achieve that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roedatacsv = pd.read_csv('/Users/peeyushtaori/Desktop/Python/roedata.csv', index_col = 'Industry Name' )\n",
    "roedatacsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roedatacsv = pd.read_csv('/Users/peeyushtaori/Desktop/Python/roedata.csv', usecols = ['Industry Name','ROE'] )\n",
    "roedatacsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capm_dem_data = pd.read_table('/Users/peeyushtaori/Desktop/Python/capm_dem.dat', delimiter=' ',header = None)\n",
    "capm_dem_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capm_dem_data = pd.read_table('/Users/peeyushtaori/Desktop/Python/capm_dem.dat', delimiter=' ',header = None)\n",
    "capm_dem_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustatdata = pd.read_csv('/Users/peeyushtaori/Desktop/Python/compustat.csv', index_col = ['ggroup','gvkey'] )\n",
    "compustatdata = compustatdata.sort_index(0,ascending=False)\n",
    "compustatdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp_data = pd.read_table('/Users/peeyushtaori/Desktop/Python/crsp.output', sep='\\s+',header = None)\n",
    "crsp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Files in Chunks**\n",
    "\n",
    "When dealing with very large files, sometimes it is handy to work with a subset of file or to work on file iteratively in smaller chunks. This can be done in pandas using chunksize and nrows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altdata = pd.read_csv('/Users/peeyushtaori/Desktop/Python/abcd.csv', chunksize=1000000 )\n",
    "altdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try not to run this\n",
    "altdata1 = pd.read_csv('/Users/peeyushtaori/Desktop/Python/abcd.csv')\n",
    "altdata1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in previous case, you did not receive a table as output. Instead you were given an object. This is called a TextParser object. This object allows you to iterate over the complete compustat file according to the chunksize you mentioned. Let us suppose we want to aggregate ebitda values of the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for chunks in altdata:\n",
    "    total += chunks['ESTIMATOR'].sum()\n",
    "    print total\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Handling missing values**\n",
    "\n",
    "Some types of missing values are automatically identified by pandas as NaN while importing the data. Those types are NA, NULL, -1.#IND. Additionally, you can also specify a list of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roemissing = pd.read_csv('/Users/peeyushtaori/Desktop/Python/roemissing.csv', na_values=['NULL',-999] )\n",
    "roemissing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roemissing = pd.read_csv('/Users/peeyushtaori/Desktop/Python/roemissing.csv', na_values={'Number of firms':['NULL',-999],'ROE':['10000.00%']} )\n",
    "roemissing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roedata = pd.read_csv('/Users/peeyushtaori/Desktop/Python/roedata.csv')\n",
    "roedata.to_csv('/Users/peeyushtaori/Desktop/Python/roedatawrite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roedata = pd.read_csv('/Users/peeyushtaori/Desktop/Python/roedata.csv')\n",
    "roedata.to_csv('/Users/peeyushtaori/Desktop/Python/roedatawrite2.csv', index=False, columns=['Industry Name','ROE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_frame = pd.DataFrame({'key': range(5), \n",
    "                           'left_value': ['a', 'b', 'c', 'd', 'e']})\n",
    "right_frame = pd.DataFrame({'key': range(2, 7), \n",
    "                           'right_value': ['f', 'g', 'h', 'i', 'j']})\n",
    "print(left_frame)\n",
    "print('\\n')\n",
    "print(right_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left_frame, right_frame, on='key', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left_frame, right_frame, on='key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left_frame, right_frame, on='key', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left_frame, right_frame, on='key', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([left_frame, right_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([left_frame, right_frame], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
